{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: Value Iteration Algorithm**"
      ],
      "metadata": {
        "id": "efXt80AZWplx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vX355SzCEMwz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.9  # Discount factor for future rewards\n",
        "theta = 1e-6  # Convergence threshold for value iteration\n",
        "grid_size = 5  # Define the size of the grid world\n",
        "\n",
        "# Initialize rewards and value function\n",
        "rewards = np.full((grid_size, grid_size), -1)  # Default reward of -1 for every step\n",
        "rewards[-1, -1] = 10  # Goal state reward of +10\n",
        "V = np.zeros((grid_size, grid_size))  # Value function initialized to zero\n",
        "\n",
        "# Actions: (dx, dy) format\n",
        "actions = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration Algorithm\n",
        "def value_iteration():\n",
        "    \"\"\"Performs value iteration to find the optimal value function.\"\"\"\n",
        "    global V\n",
        "    while True:\n",
        "        delta = 0  # Track maximum change in value function\n",
        "        new_V = np.copy(V)\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                if (i, j) == (grid_size-1, grid_size-1):\n",
        "                    continue  # Skip goal state since its value is fixed\n",
        "\n",
        "                max_value = float('-inf')  # Store best action value\n",
        "                for action, (di, dj) in actions.items():\n",
        "                    ni, nj = i + di, j + dj  # Next state\n",
        "                    if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                        value = rewards[i, j] + gamma * V[ni, nj]  # Bellman update\n",
        "                        max_value = max(max_value, value)\n",
        "                new_V[i, j] = max_value\n",
        "                delta = max(delta, abs(new_V[i, j] - V[i, j]))  # Track max change\n",
        "        V = new_V\n",
        "        if delta < theta:  # Stop when convergence is achieved\n",
        "            break\n",
        "\n",
        "\n",
        "# Run value iteration\n",
        "value_iteration()\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V)"
      ],
      "metadata": {
        "id": "BcCXywjd8B7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2608015-57c8-4791-8c8b-3ef08b9d92e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[-5.6953279 -5.217031  -4.68559   -4.0951    -3.439    ]\n",
            " [-5.217031  -4.68559   -4.0951    -3.439     -2.71     ]\n",
            " [-4.68559   -4.0951    -3.439     -2.71      -1.9      ]\n",
            " [-4.0951    -3.439     -2.71      -1.9       -1.       ]\n",
            " [-3.439     -2.71      -1.9       -1.         0.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract policy\n",
        "def extract_policy():\n",
        "    \"\"\"Extracts the optimal policy from the value function.\"\"\"\n",
        "    policy = np.empty((grid_size, grid_size), dtype=object)\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == (grid_size-1, grid_size-1):\n",
        "                policy[i, j] = 'G'  # Mark goal state\n",
        "                continue\n",
        "\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "            for action, (di, dj) in actions.items():\n",
        "                ni, nj = i + di, j + dj\n",
        "                if 0 <= ni < grid_size and 0 <= nj < grid_size:\n",
        "                    value = rewards[i, j] + gamma * V[ni, nj]\n",
        "                    if value > best_value:\n",
        "                        best_value = value\n",
        "                        best_action = action[0].upper()  # Store best action\n",
        "            policy[i, j] = best_action\n",
        "    return policy\n",
        "\n",
        "policy = extract_policy()\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpR-NFUWWIOB",
        "outputId": "b040322c-e775-4dee-afeb-d339165d0ac0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[['D' 'D' 'D' 'D' 'D']\n",
            " ['D' 'D' 'D' 'D' 'D']\n",
            " ['D' 'D' 'D' 'D' 'D']\n",
            " ['D' 'D' 'D' 'D' 'D']\n",
            " ['R' 'R' 'R' 'R' 'G']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Q-Learning Algorithm**"
      ],
      "metadata": {
        "id": "fZ2d0xAmWYDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning Algorithm\n",
        "alpha = 0.5  # Learning rate\n",
        "epsilon = 0.1  # Exploration factor\n",
        "episodes = 5000  # Number of training episodes\n",
        "Q = np.zeros((grid_size, grid_size, len(actions)))  # Initialize Q-table with zeros\n",
        "\n",
        "# Convert action names to indices\n",
        "action_list = list(actions.keys())"
      ],
      "metadata": {
        "id": "LrQ9dyJ1UJTr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_action(state):\n",
        "    \"\"\"Chooses an action using the epsilon-greedy strategy.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(actions))  # Explore: random action\n",
        "    return np.argmax(Q[state[0], state[1]])  # Exploit: best known action\n",
        "\n",
        "def q_learning():\n",
        "    \"\"\"Performs Q-learning to find the optimal action-value function.\"\"\"\n",
        "    for _ in range(episodes):\n",
        "        state = (0, 0)  # Start at top-left corner\n",
        "        while state != (grid_size-1, grid_size-1):  # Until reaching goal state\n",
        "            action_idx = choose_action(state)  # Choose an action\n",
        "            action = action_list[action_idx]\n",
        "            di, dj = actions[action]  # Get action direction\n",
        "            new_state = (state[0] + di, state[1] + dj)\n",
        "            if not (0 <= new_state[0] < grid_size and 0 <= new_state[1] < grid_size):\n",
        "                new_state = state  # Stay in place if out of bounds\n",
        "\n",
        "            reward = rewards[state[0], state[1]]  # Get reward for current state\n",
        "            best_next_q = np.max(Q[new_state[0], new_state[1]])  # Best future Q-value\n",
        "\n",
        "            # Q-learning update rule\n",
        "            Q[state[0], state[1], action_idx] += alpha * (reward + gamma * best_next_q - Q[state[0], state[1], action_idx])\n",
        "\n",
        "            state = new_state  # Move to next state\n",
        "\n",
        "# Run Q-learning\n",
        "q_learning()\n",
        "print(\"Learned Q-Values:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqBbJq1zV_PR",
        "outputId": "d9463f00-2c69-4524-b8d9-5563a2956674"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-Values:\n",
            "[[[-6.12579511 -5.6953279  -6.12579511 -5.6953279 ]\n",
            "  [-5.69501728 -5.217031   -6.10014895 -5.217031  ]\n",
            "  [-5.04180508 -4.68559    -5.65081423 -4.68559   ]\n",
            "  [-4.65663594 -4.0951     -5.17034904 -4.0951    ]\n",
            "  [-3.81774001 -3.439      -4.27823847 -4.03083014]]\n",
            "\n",
            " [[-6.12579511 -5.217031   -5.6953279  -5.217031  ]\n",
            "  [-5.69524117 -4.68559    -5.68593936 -4.68559   ]\n",
            "  [-4.93652236 -4.0951     -4.88028184 -4.0951    ]\n",
            "  [-4.67081451 -3.439      -4.42501667 -3.439     ]\n",
            "  [-3.85592925 -2.71       -3.9520281  -3.38254162]]\n",
            "\n",
            " [[-5.6953279  -4.68559    -5.217031   -4.68559   ]\n",
            "  [-5.21596547 -4.0951     -5.2169172  -4.0951    ]\n",
            "  [-4.56703604 -3.439      -4.61394308 -3.439     ]\n",
            "  [-4.05661431 -2.71       -3.81893422 -2.71      ]\n",
            "  [-3.18356468 -1.9        -3.43815674 -2.6927417 ]]\n",
            "\n",
            " [[-5.217031   -4.0951     -4.68559    -4.0951    ]\n",
            "  [-4.68558158 -3.439      -4.68558692 -3.439     ]\n",
            "  [-4.09345105 -2.71       -4.09349181 -2.71      ]\n",
            "  [-3.43375367 -1.9        -3.42952386 -1.9       ]\n",
            "  [-2.67113011 -1.         -2.66433664 -1.89259766]]\n",
            "\n",
            " [[-4.68559    -4.0951     -4.0951     -3.439     ]\n",
            "  [-4.0951     -3.439      -4.0951     -2.71      ]\n",
            "  [-3.439      -2.71       -3.439      -1.9       ]\n",
            "  [-2.71       -1.9        -2.71       -1.        ]\n",
            "  [ 0.          0.          0.          0.        ]]]\n"
          ]
        }
      ]
    }
  ]
}